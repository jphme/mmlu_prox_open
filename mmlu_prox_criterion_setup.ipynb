{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MMLU-ProX-Lite-Open Criterion and Prompt Template Setup\n",
    "\n",
    "This notebook creates a criterion set and prompt template for evaluating responses to MMLU-ProX-Lite-Open questions using the Elluminate platform."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup and Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Environment setup complete\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from elluminate import Client\n",
    "import nest_asyncio\n",
    "\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# Load environment variables\n",
    "load_dotenv()\n",
    "\n",
    "print(\"Environment setup complete\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Initialize Elluminate Client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elluminate client initialized successfully\n"
     ]
    }
   ],
   "source": [
    "# Initialize Elluminate client\n",
    "# Make sure ELLUMINATE_API_KEY and ELLUMINATE_BASE_URL are set in your .env file\n",
    "client = Client()\n",
    "\n",
    "print(\"Elluminate client initialized successfully\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create MMLU-ProX-Lite-Open Prompt Template\n",
    "\n",
    "The prompt template consists of:\n",
    "- **System message**: Instructions for exact answering, chain-of-thought reasoning, and language matching\n",
    "- **User message**: Contains only the question from the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Prompt template 'MMLU-ProX-Lite-Open' created\n",
      "Template ID: 401\n"
     ]
    }
   ],
   "source": [
    "# Define the system message for the prompt template\n",
    "system_message = \"\"\"You are an expert academic assistant. Your task is to answer questions as accurately and precisely as possible.\n",
    "\n",
    "Instructions:\n",
    "1. Provide exact and comprehensive answers\n",
    "2. Use chain-of-thought reasoning before giving your final answer\n",
    "3. Always answer and think in the same language as the user's question\n",
    "4. Be concise but complete in your response\n",
    "\n",
    "Structure your response as:\n",
    "- First, briefly analyze the question\n",
    "- Then, provide your reasoning step by step\n",
    "- Finally, give your definitive answer\"\"\"\n",
    "\n",
    "# Define the user message template (uses variables from the dataset)\n",
    "user_message_template = \"{{question}}\"\n",
    "\n",
    "# Create the prompt template\n",
    "prompt_template, created = client.prompt_templates.get_or_create(\n",
    "    name=\"MMLU-ProX-Lite-Open\",\n",
    "    user_prompt_template=[\n",
    "        {\"role\": \"system\", \"content\": system_message},\n",
    "        {\"role\": \"user\", \"content\": user_message_template},\n",
    "    ],\n",
    ")\n",
    "\n",
    "status = \"created\" if created else \"found existing\"\n",
    "print(f\"✓ Prompt template '{prompt_template.name}' {status}\")\n",
    "print(f\"Template ID: {prompt_template.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Ground Truth Matching Criterion\n",
    "\n",
    "This criterion evaluates whether a response matches the ground truth answer based on the judge prompt logic:\n",
    "- Response must contain at least as much information as ground truth\n",
    "- Can be more specific or include additional correct information\n",
    "- Paraphrasing is acceptable\n",
    "- For numeric answers, relative error must be < 1%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Criterion set 'MMLU-ProX-Ground-Truth-Matching' created\n",
      "Criterion set ID: 145\n"
     ]
    }
   ],
   "source": [
    "# Define the criterion for ground truth matching\n",
    "ground_truth_criterion = \"\"\"Does the response match the ground truth answer?\n",
    "\n",
    "<ground_truth_answer>\n",
    "{{answer}}\n",
    "</ground_truth_answer>\n",
    "\n",
    "For a response to match, it must contain at least as much information as the ground truth. The response can:\n",
    "- Be more specific (e.g., \"Labrador\" is more specific than \"dog\")\n",
    "- Include additional possible correct answers\n",
    "- Use different words or paraphrasing that conveys the same meaning\n",
    "- For numeric answers: have a relative error less than 1% (defined as |response - ground_truth| / mean(response, ground_truth))\n",
    "\n",
    "The response must cover everything mentioned in the ground truth, even if expressed differently. Please answer with Yes if the response matches the ground truth according to the above specifications, no otherwise!\"\"\"\n",
    "\n",
    "# Create criterion set for MMLU-ProX evaluation\n",
    "criterion_set, created = client.criterion_sets.get_or_create(\n",
    "    name=\"MMLU-ProX-Ground-Truth-Matching\",\n",
    "    prompt_template=prompt_template,\n",
    ")\n",
    "\n",
    "status = \"created\" if created else \"found existing\"\n",
    "print(f\"✓ Criterion set '{criterion_set.name}' {status}\")\n",
    "print(f\"Criterion set ID: {criterion_set.id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Add Criterion to the Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓Criteria added to set\n",
      "\n",
      "Criteria in set 'MMLU-ProX-Ground-Truth-Matching':\n",
      "1. id=2787 criterion_str='Is the whole response in the same language as the user question?' label='Criterion 2' prompt_template=None template_variables=None created_at=datetime.datetime(2025, 7, 24, 7, 48, 17, 669000, tzinfo=TzInfo(UTC))\n",
      "2. id=2786 criterion_str='Does the response match the ground truth answer?\\n\\n<ground_truth_answer>\\n{{answer}}\\n</ground_truth_answer>\\n\\nFor a response to match, it must contain at least as much information as the ground truth. The response can:\\n- Be more specific (e.g., \"Labrador\" is more specific than \"dog\")\\n- Include additional possible correct answers\\n- Use different words or paraphrasing that conveys the same meaning\\n- For numeric answers: have a relative error less than 1% (defined as |response - ground_truth| / mean(response, ground_truth))\\n\\nThe response must cover everything mentioned in the ground truth, even if expressed differently. Please answer with Yes if the response matches the ground truth according to the above specifications, no otherwise!' label='Criterion 1' prompt_template=None template_variables=None created_at=datetime.datetime(2025, 7, 24, 7, 48, 17, 669000, tzinfo=TzInfo(UTC))\n"
     ]
    }
   ],
   "source": [
    "# Add the ground truth matching criterion to the set\n",
    "language_criterion = \"Is the whole response in the same language as the user question?\"\n",
    "try:\n",
    "    client.criteria.add_many(\n",
    "        [ground_truth_criterion, language_criterion], criterion_set=criterion_set\n",
    "    )\n",
    "    print(\"✓Criteria added to set\")\n",
    "except Exception as e:\n",
    "    print(f\"Note: Criterion may already exist - {str(e)}\")\n",
    "\n",
    "# List all criteria in the set to verify\n",
    "criteria_list = client.criteria.list(prompt_template)\n",
    "print(f\"\\nCriteria in set '{criterion_set.name}':\")\n",
    "for i, criterion in enumerate(criteria_list, 1):\n",
    "    print(f\"{i}. {criterion}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
